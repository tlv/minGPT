% === Core References ===

@misc{karpathy2022mingpt,
  author       = {Andrej Karpathy},
  title        = {minGPT},
  year         = {2022},
  howpublished = {\url{https://github.com/karpathy/minGPT}},
}

@article{eldan2023tinystories,
  title        = {TinyStories: How Small Can Language Models Be and Still Speak Coherent English?},
  author       = {Eldan, Ronen and Li, Yuanzhi},
  journal      = {arXiv preprint arXiv:2305.07759},
  year         = {2023},
}

% === Anthropic Safety/Interpretability Papers ===

@online{anthropic2024sleeper,
author = {Monte MacDiarmid and Timothy Maxwell and Nicholas Schiefer and Jesse Mu and Jared Kaplan and David Duvenaud and Sam Bowman and Alex Tamkin and Ethan Perez and Mrinank Sharma and Carson Denison and Evan Hubinger},
title = {Simple probes can catch sleeper agents},
date = {2024-04-23},
year = {2024},
url = {https://www.anthropic.com/news/probes-catch-sleeper-agents},
}

@article{marks2025auditing,
  title        = {Auditing language models for hidden objectives},
  author       = {Marks, Samuel and Treutlein, Johannes and Bricken, Trenton and others},
  journal      = {arXiv preprint arXiv:2503.10965},
  year         = {2025},
}

@article{hubinger2024sleeper,
  title        = {Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training},
  author       = {Hubinger, Evan and others},
  journal      = {arXiv preprint arXiv:2401.05566},
  year         = {2024},
}

% === Mechanistic Interpretability ===

@article{elhage2021mathematical,
  title        = {A Mathematical Framework for Transformer Circuits},
  author       = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and others},
  journal      = {Transformer Circuits Thread},
  year         = {2021},
  howpublished = {\url{https://transformer-circuits.pub/2021/framework/index.html}},
}

@article{olsson2022context,
  title        = {In-context Learning and Induction Heads},
  author       = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and others},
  journal      = {Transformer Circuits Thread},
  year         = {2022},
}

@article{nanda2023progress,
  title        = {Progress measures for grokking via mechanistic interpretability},
  author       = {Nanda, Neel and Chan, Lawrence and Liberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal      = {arXiv preprint arXiv:2301.05217},
  year         = {2023},
}

@article{bricken2023monosemanticity,
  title        = {Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author       = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and others},
  journal      = {Transformer Circuits Thread},
  year         = {2023},
  howpublished = {\url{https://transformer-circuits.pub/2023/monosemantic-features}},
}

@misc{rumbelow2023magikarp,
  author       = {Rumbelow, Jessica and Watkins, Matthew},
  title        = {SolidGoldMagikarp (plus, prompt generation)},
  year         = {2023},
  howpublished = {\url{https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation}},
}

@misc{nostalgebraist2020logitlens,
  author       = {nostalgebraist},
  title        = {interpreting GPT: the logit lens},
  year         = {2020},
  howpublished = {\url{https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}},
}

% === Foundational Transformer Papers ===

@inproceedings{vaswani2017attention,
  title        = {Attention is All You Need},
  author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
  booktitle    = {Advances in Neural Information Processing Systems},
  year         = {2017},
}

@article{radford2018improving,
  title        = {Improving Language Understanding by Generative Pre-Training},
  author       = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal      = {OpenAI Blog},
  year         = {2018},
}

@article{radford2019language,
  title        = {Language Models are Unsupervised Multitask Learners},
  author       = {Radford, Alec and Wu, Jeffrey and Child, Rewon and others},
  journal      = {OpenAI Blog},
  year         = {2019},
}

@article{ba2016layer,
  title        = {Layer Normalization},
  author       = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  journal      = {arXiv preprint arXiv:1607.06450},
  year         = {2016},
}

% === Backdoor Attacks ===

@inproceedings{gu2017badnets,
  title        = {BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  author       = {Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  booktitle    = {arXiv preprint arXiv:1708.06733},
  year         = {2017},
}

@inproceedings{chen2021badnl,
  title        = {BadNL: Backdoor Attacks Against NLP Models with Semantic-preserving Improvements},
  author       = {Chen, Xiaoyi and Salem, Ahmed and Chen, Dingfan and others},
  booktitle    = {Annual Computer Security Applications Conference},
  year         = {2021},
}

@article{wallace2021concealed,
  title        = {Concealed Data Poisoning Attacks on NLP Models},
  author       = {Wallace, Eric and Zhao, Tony and Feng, Shi and Singh, Sameer},
  journal      = {arXiv preprint arXiv:2010.12563},
  year         = {2021},
}

% === AI Safety ===

@article{ngo2022alignment,
  title        = {The Alignment Problem from a Deep Learning Perspective},
  author       = {Ngo, Richard and Chan, Lawrence and Mindermann, S{\"o}ren},
  journal      = {arXiv preprint arXiv:2209.00626},
  year         = {2022},
}

@misc{amodei2025urgency,
  author       = {Amodei, Dario},
  title        = {The Urgency of Interpretability},
  year         = {2025},
  howpublished = {\url{https://www.darioamodei.com/post/the-urgency-of-interpretability}},
}

% === Backdoor Detection ===

@inproceedings{xu2021metatrojan,
  title        = {Detecting AI Trojans Using Meta Neural Analysis},
  author       = {Xu, Xiaojun and Wang, Qi and Li, Huichen and Borisov, Nikita and Gunter, Carl A. and Li, Bo},
  booktitle    = {IEEE Symposium on Security and Privacy},
  year         = {2021},
}

@article{kolouri2019ulp,
  title        = {Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs},
  author       = {Kolouri, Soheil and Saha, Aniruddha and Pirsiavash, Hamed and Hoffmann, Heiko},
  journal      = {arXiv preprint arXiv:1906.10842},
  year         = {2019},
}

@inproceedings{wang2019neural,
  title        = {Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks},
  author       = {Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y.},
  booktitle    = {IEEE Symposium on Security and Privacy},
  year         = {2019},
}
